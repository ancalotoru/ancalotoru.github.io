@article{Radovanovic2015,
abstract = {Outlier detection in high-dimensional data presents vari ous challenges resulting from the “curse of dimensionality .” A prevailing view is that distance concentration, i.e., the tendency of distances in high-dimensional data to become in discernible, hinders the detection of outliers by making distance-based methods label all points as almost equally good outliers. In this paper we provide evidence supporting the opinion that such a view i s too simple, by demonstrating that distance-based methods can produce more contrasting outlier scores in high-dimension al settings. Furthermore, we show that high dimensionality can have a different impact, by reexamining the notion of reverse near est neighbors in the unsupervised outlier-detection conte xt. Namely, it was recently observed that the distribution of points' reve rse-neighbor counts becomes skewed in high dimensions, res ulting in the phenomenon known as hubness . We provide insight into how some points (antihubs) appear v ery infrequently in k -NN lists of other points, and explain the connection between antihub s, outliers, and existing unsupervised outlier-detection methods. By evaluating the classic k -NN method, the angle-based technique (ABOD) designed for h igh-dimensional data, the density-based local outlier factor (LOF) and influenced outlierness (INFL O) methods, and antihub-based methods on various synthetic and real-world data sets, we offer novel insight into the useful ness of reverse neighbor counts in unsupervised outlier det ection},
author = {Radovanovi{\'{c}}, Milo{\v{s}} and Nanopoulos, Alexandros and Ivanovi{\'{c}}, Mirjana},
doi = {10.1109/TKDE.2014.2365790},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Outlier detection,distance concentration,high-dimensional data,reverse nearest neighbors},
title = {{Reverse nearest neighbors in unsupervised distance-based outlier detection}},
year = {2015}
}
@article{Campos2016,
abstract = {The evaluation of unsupervised outlier detection algorithms is a constant challenge in data mining research. Little is known regarding the strengths and weak-nesses of different standard outlier detection models, and the impact of parameter G. O. Campos et al. choices for these algorithms. The scarcity of appropriate benchmark datasets with ground truth annotation is a significant impediment to the evaluation of outlier meth-ods. Even when labeled datasets are available, their suitability for the outlier detection task is typically unknown. Furthermore, the biases of commonly-used evaluation mea-sures are not fully understood. It is thus difficult to ascertain the extent to which newly-proposed outlier detection methods improve over established methods. In this paper, we perform an extensive experimental study on the performance of a represen-tative set of standard k nearest neighborhood-based methods for unsupervised outlier detection, across a wide variety of datasets prepared for this purpose. Based on the overall performance of the outlier detection methods, we provide a characterization of the datasets themselves, and discuss their suitability as outlier detection benchmark sets. We also examine the most commonly-used measures for comparing the perfor-mance of different methods, and suggest adaptations that are more suitable for the evaluation of outlier detection results.},
author = {Campos, Guilherme O. and Zimek, Arthur and Sander, J{\~{A}}{\P}rg and Campello, Ricardo J.G.B. and Micenkov{\~{A}}¡, Barbora and Schubert, Erich and Assent, Ira and Houle, Michael E.},
doi = {10.1007/s10618-015-0444-8},
isbn = {1061801504448},
issn = {1573756X},
journal = {Data Mining and Knowledge Discovery},
keywords = {Datasets,Evaluation,Measures,Unsupervised outlier detection},
title = {{On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study}},
year = {2016}
}
@article{Auffray2014,
abstract = {Bounding probabilities of rare events in the context of computer experiments is an important concern in reliability studies. These rare events depend on the output of a physical model with random input variables. Since the model is only known through an expensive black box function, standard efficient Monte Carlo methods designed for rare events cannot be used. That is why a strategy based on importance sampling methods is proposed. This strategy relies on Kriging meta-modeling and manages to achieve sharp upper confidence bounds on the rare events probabilities. The variability due to the Kriging meta-modeling step is properly taken into account. The proposed methodology is applied to an artificial example and compared with more standard Bayesian bounds. Eventually, a challenging real case is analyzed. It consists in finding an upper bound for the probability that the trajectory of an airborne load will collide with the aircraft that released it. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1105.0871},
author = {Auffray, Yves and Barbillon, Pierre and Marin, Jean Michel},
doi = {10.1016/j.csda.2014.06.023},
eprint = {1105.0871},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Bayesian estimates,Computer experiments,Importance sampling,Kriging,Rare events,Risk assessment with fighter aircraft},
title = {{Bounding rare event probabilities in computer experiments}},
year = {2014}
}
@article{Bourinet2016,
abstract = {{\textcopyright} 2016 Elsevier Ltd. All rights reserved. Assessing rare event probabilities still suffers from its computational cost despite some available methods widely accepted by researchers and engineers. For low to moderately high dimensional problems and under the assumption of a smooth limit-state function, adaptive strategies based on surrogate models represent interesting alternative solutions. This paper presents such an adaptive method based on support vector machine surrogates used in regression. The key idea is to iteratively construct surrogates which quickly explore the safe domain and focus on the limit-state surface in its final stage. Highly accurate surrogates are constructed at each iteration by minimizing an estimation of the leave-one-out error with the cross-entropy method. Additional training points are generated with the Metropolis-Hastings algorithm modified by Au and Beck and a local kernel regression is made over a subset of the known data. The efficiency of the method is tested on examples featuring various challenges: a highly curved limit-state surface at a single most probable failure point, a smooth high-dimensional limit-state surface and a parallel system.},
author = {Bourinet, J.-M.},
doi = {10.1016/j.ress.2016.01.023},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Adaptive surrogate models,Hyperparameter selection,Rare events,Regression,Reliability assessment,Span bound approximation,Support vector machines},
title = {{Rare-event probability estimation with adaptive support vector regression surrogates}},
year = {2016}
}
@article{VanDenEeckhaut2006,
abstract = {In this article a statistical multivariate method, i.e., rare events logistic regression, is evaluated for the creation of a landslide susceptibility map in a 200 km2study area of the Flemish Ardennes (Belgium). The methodology is based on the hypothesis that future landslides will have the same causal factors as the landslides initiated in the past. The information on the past landslides comes from a landslide inventory map obtained by detailed field surveys and by the analysis of LIDAR (Light Detection and Ranging)-derived hillshade maps. Information on the causal factors (e.g., slope gradient, aspect, lithology, and soil drainage) was extracted from digital elevation models derived from LIDAR and from topographical, lithological and soil maps. In landslide-affected areas, however, we did not use the present-day hillslope gradient. In order to reflect the hillslope condition prior to landsliding, the pre-landslide hillslope was reconstructed and its gradient was used in the analysis. Because of their limited spatial occurrence, the landslides in the study area can be regarded as "rare events". Rare events logistic regression differs from ordinary logistic regression because it takes into account the low proportion of 1s (landslides) to 0s (no landslides) in the study area by incorporating three correction measures: the endogenous stratified sampling of the dataset, the prior correction of the intercept and the correction of the probabilities to include the estimation uncertainty. For the study area, significant model results were obtained, with pre-landslide hillslope gradient and three different clayey lithologies being important predictor variables. Receiver Operating Characteristic (ROC) curves and the Kappa index were used to validate the model. Both show a good agreement between the observed and predicted values of the validation dataset. Based on a qualified judgement, the created landslide susceptibility map was classified into four classes, i.e., very high, high, moderate and low susceptibility. If interpreted correctly, this classified susceptibility map is an important tool for the delineation of zones where prevention measures are needed and human interference should be limited in order to avoid property damage due to landslides. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {{Van Den Eeckhaut}, M. and Vanwalleghem, T. and Poesen, J. and Govers, G. and Verstraeten, G. and Vandekerckhove, L.},
doi = {10.1016/j.geomorph.2005.12.003},
isbn = {0169-555X},
issn = {0169555X},
journal = {Geomorphology},
keywords = {Deep-seated landslides,Landslide susceptibility map,Pre-landslide hillslope gradient,Rare events logistic regression},
title = {{Prediction of landslide susceptibility using rare events logistic regression: A case-study in the Flemish Ardennes (Belgium)}},
year = {2006}
}
@article{Ren2016,
abstract = {Red light running (RLR) has become a major safety concern at signalized intersection. To prevent RLR related crashes, it is critical to identify the factors that significantly impact the drivers' behaviors of RLR, and to predict potential RLR in real time. In this research, 9-month's RLR events extracted from high-resolution traffic data collected by loop detectors from three signalized intersections were applied to identify the factors that significantly affect RLR behaviors. The data analysis indicated that occupancy time, time gap, used yellow time, time left to yellow start, whether the preceding vehicle runs through the intersection during yellow, and whether there is a vehicle passing through the intersection on the adjacent lane were significantly factors for RLR behaviors. Furthermore, due to the rare events nature of RLR, a modified rare events logistic regression model was developed for RLR prediction. The rare events logistic regression method has been applied in many fields for rare events studies and shows impressive performance, but so far none of previous research has applied this method to study RLR. The results showed that the rare events logistic regression model performed significantly better than the standard logistic regression model. More importantly, the proposed RLR prediction method is purely based on loop detector data collected from a single advance loop detector located 400 feet away from stop-bar. This brings great potential for future field applications of the proposed method since loops have been widely implemented in many intersections and can collect data in real time. This research is expected to contribute to the improvement of intersection safety significantly.},
author = {Ren, Yilong and Wang, Yunpeng and Wu, Xinkai and Yu, Guizhen and Ding, Chuan},
doi = {10.1016/j.aap.2016.07.017},
issn = {00014575},
journal = {Accident Analysis and Prevention},
keywords = {High-resolution traffic data,Influential factors,Logistic regression,Rare events,Red light running},
pmid = {1612279},
title = {{Influential factors of red-light running at signalized intersection and prediction using a rare events logistic regression model}},
year = {2016}
}
@article{Xu2016,
abstract = {Video surveillance infrastructure has been widely installed in public places for security purposes. However, live video feeds are typically monitored by human staff, making the detection of important events as they occur difficult. As such, an expert system that can automatically detect events of interest in surveillance footage is highly desirable. Although a number of approaches have been proposed, they have significant limitations: supervised approaches, which can detect a specific event, ideally require a large number of samples with the event spatially and temporally localised; while unsupervised approaches, which do not require this demanding annotation, can only detect whether an event is abnormal and not specific event types. To overcome these problems, we formulate a weakly-supervised approach using Kullback-Leibler (KL) divergence to detect rare events. The proposed approach leverages the sparse nature of the target events to its advantage, and we show that this data imbalance guarantees the existence of a decision boundary to separate samples that contain the target event from those that do not. This trait, combined with the coarse annotation used by weakly supervised learning (that only indicates approximately when an event occurs), greatly reduces the annotation burden while retaining the ability to detect specific events. Furthermore, the proposed classifier requires only a decision threshold, simplifying its use compared to other weakly supervised approaches. We show that the proposed approach outperforms state-of-the-art methods on a popular real-world traffic surveillance dataset, while preserving real time performance.},
author = {Xu, Jingxin and Denman, Simon and Fookes, Clinton and Sridharan, Sridha},
doi = {10.1016/j.eswa.2016.01.035},
isbn = {9781467369978},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Anomaly detection,Event detection,Kullback-Leibler divergence,Weakly supervised learning},
title = {{Detecting rare events using Kullback-Leibler divergence: A weakly supervised approach}},
year = {2016}
}
@article{Balesdent2016,
abstract = {The accurate estimation of rare event probabilities is a crucial problem in engineering to characterize the reliability of complex systems. Several methods such as Importance Sampling or Importance Splitting have been proposed to perform the estimation of such events more accurately (i.e., with a lower variance) than crude Monte Carlo method. However, these methods assume that the probability distributions of the input variables are exactly defined (e.g., mean and covariance matrix perfectly known if the input variables are defined through Gaussian laws) and are not able to determine the impact of a change in the input distribution parameters on the probability of interest. The problem considered in this paper is the propagation of the input distribution parameter uncertainty defined by intervals to the rare event probability. This problem induces intricate optimization and numerous probability estimations in order to determine the upper and lower bounds of the probability estimate. The calculation of these bounds is often numerically intractable for rare event probability (say 10-5), due to the high computational cost required. A new methodology is proposed to solve this problem with a reduced simulation budget, using the adaptive Importance Sampling. To this end, a method for estimating the Importance Sampling optimal auxiliary distribution is proposed, based on preceding Importance Sampling estimations. Furthermore, a Kriging-based adaptive Importance Sampling is used in order to minimize the number of evaluations of the computationally expensive simulation code. To determine the bounds of the probability estimate, an evolutionary algorithm is employed. This algorithm has been selected to deal with noisy problems since the Importance Sampling probability estimate is a random variable. The efficiency of the proposed approach, in terms of accuracy of the found results and computational cost, is assessed on academic and engineering test cases. {\textcopyright} 2014 Springer Science+Business Media New York.},
author = {Balesdent, Mathieu and Morio, J{\'{e}}r{\^{o}}me and Brevault, Lo{\"{i}}c},
doi = {10.1007/s11009-014-9411-x},
isbn = {1387-5841},
issn = {15737713},
journal = {Methodology and Computing in Applied Probability},
keywords = {Epistemic uncertainty,Importance sampling,Input–Output function,Kriging,Rare event estimation,Surrogate model},
title = {{Rare Event Probability Estimation in the Presence of Epistemic Uncertainty on Input Probability Distribution Parameters}},
year = {2016}
}
@inproceedings{Theofilatos2016,
abstract = {Modeling road accident occurrence has gained increasing attention over the years. So far, considerable efforts have been made from researchers and policy makers in order to explain road accidents and improve road safety performance of highways. In reality, road accidents are rare events. In such cases, the binary dependent variable is characterized by dozens to thousands of times fewer events (accidents) than non-events (non-accidents). Instead of using traditional logistic regression methods, this paper considers accidents as rare events and proposes a series of rare-events logit models which are applied in order to model road accident occurrence by utilizing real-time traffic data. This statistical procedure was initially proposed by King and Zeng (2001) when scholars study rare events such as wars, massive economic crises and so on. Rare-events logit models basically estimate the same models as traditional logistic regression, but the estimates as well as the probabilities are corrected for the bias that occurs when the sample is small or the observed events are very rare. Consequently, the basic problem of underestimating the event probabilities is avoided as stated by King and Zeng (2001). To the best of our knowledge, this is the first time that this approach is followed when road accident data are analyzed. Instead of applying a traditional case-control study, the complete dataset of hourly aggregated traffic data such as flow, occupancy, mean time speed and percentage of trucks, were collected from three random loop detectors in the Attica Tollway ("Attiki Odos") located in Greater Athens Area in Greece for the 2008-2011 period. The modeling results showed an adequate statistical fit and reveal a negative relationship between accident occurrence and the natural logarithm of speed in the accident location. This study attempts to contribute to the understanding of accident occurrence in motorways by developing novel models such as the rare-events logit for the first time in safety evaluation of motorways.},
author = {Theofilatos, Athanasios and Yannis, George and Kopelias, Pantelis and Papadimitriou, Fanis},
booktitle = {Transportation Research Procedia},
doi = {10.1016/j.trpro.2016.05.293},
issn = {23521465},
keywords = {accident occurrence,logistic regression,rare events,traffic parameters},
title = {{Predicting Road Accidents: A Rare-events Modeling Approach}},
year = {2016}
}
@article{Dzierma2010,
abstract = {Probabilistic forecasting of volcanic eruptions is a central issue of applied volcanology with regard to mitigating consequences of volcanic hazards. Recent years have seen great advances in the techniques of statistical analysis of volcanic eruption time series, which constitutes an essential component of a multi-discipline volcanic hazard assessment. Here, two of the currently most active volcanoes of South America, Villarrica and Llaima, are subjected to an established statistical procedure, with the aim to provide predictions for the likelihood of future eruptions within a given time interval.In the eruptive history of both Villarrica and Llaima Volcanoes, time independence of eruptions provides consistency with Poissonian behaviour. A moving-average test, helping to assess whether the distribution of repose times between eruptions changes in response to the time interval considered, validates stationarity for at least the younger eruption record. For the earlier time period, stationarity is not entirely confirmed, which may artificially result from incompleteness of the eruption record, but can also reveal fluctuations in the eruptive regime. To take both possibilities into account, several different distribution functions are fit to the eruption time series, and the fits are evaluated for their quality and compared. The exponential, Weibull and log-logistic distributions are shown to fit the repose times sufficiently well. The probability of future eruptions within defined time periods is therefore estimated from all three distribution functions, as well as from a mixture of exponential distribution (MOED) for the different eruption regimes and from a Bayesian approach. Both the MOED and Bayesian estimates intrinsically predict lower eruption probabilities than the exponential distribution function, while the Weibull distributions have increasing hazard rates, hence giving the highest eruption probability forecasts.This study provides one of the first approaches to subject historical time series of small eruptions (including those of Volcanic Explosivity Index = 2) of very active volcanoes to this type of statistical analysis. Since both Villarrica and Llaima are situated in a region of high population density, the eruption probabilities determined in this study present a valuable contribution to regional hazard assessment. ?? 2010 Elsevier B.V.},
author = {Dzierma, Yvonne and Wehrmann, Heidi},
doi = {10.1016/j.jvolgeores.2010.03.009},
isbn = {0377-0273},
issn = {03770273},
journal = {Journal of Volcanology and Geothermal Research},
keywords = {Distribution functions,Eruption time series,Llaima Volcano,Probabilistic eruption forecasting,Villarrica Volcano,Volcanic hazards},
title = {{Eruption time series statistically examined: Probabilities of future eruptions at Villarrica and Llaima Volcanoes, Southern Volcanic Zone, Chile}},
year = {2010}
}
@article{Straub2016,
abstract = {In many areas of engineering and science there is an interest in predicting the probability of rare events, in particular in applications related to safety and security. Increasingly, such predictions are made through computer models of physical systems in an uncertainty quantification framework. Additionally, with advances in IT, monitoring and sensor technology, an increasing amount of data on the performance of the systems is collected. This data can be used to reduce uncertainty, improve the probability estimates and consequently enhance the management of rare events and associated risks. Bayesian analysis is the ideal method to include the data into the probabilistic model. It ensures a consistent probabilistic treatment of uncertainty, which is central in the prediction of rare events, where extrapolation from the domain of observation is common. We present a framework for performing Bayesian updating of rare event probabilities, termed BUS. It is based on a reinterpretation of the classical rejection-sampling approach to Bayesian analysis, which enables the use of established methods for estimating probabilities of rare events. By drawing upon these methods, the framework makes use of their computational efficiency. These methods include the First-Order Reliability Method (FORM), tailored importance sampling (IS) methods and Subset Simulation (SuS). In this contribution, we briefly review these methods in the context of the BUS framework and investigate their applicability to Bayesian analysis of rare events in different settings. We find that, for some applications, FORM can be highly efficient and is surprisingly accurate, enabling Bayesian analysis of rare events with just a few model evaluations. In a general setting, BUS implemented through IS and SuS is more robust and flexible.},
author = {Straub, Daniel and Papaioannou, Iason and Betz, Wolfgang},
doi = {10.1016/j.jcp.2016.03.018},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Bayesian analysis,Importance sampling,Rare events,Reliability,Subset simulation},
title = {{Bayesian analysis of rare events}},
year = {2016}
}
@article{Murray2005,
abstract = {We compare machine learning methods applied to a difficult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classifier (mi-NB) which is specifically designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum significance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures.},
author = {Murray, Joseph F and Hughes, Gordon F and Kreutz-Delgado, Kenneth},
doi = {10.1.1.84.9557},
isbn = {1532-4435},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {exact,hard drive failure prediction,multiple instance naive-bayes,nonparametric statistics,rank-sum test,support vector machines,svm},
title = {{Machine Learning Methods for Predicting Failures in Hard Drives: A Multiple-Instance Application}},
year = {2005}
}
@article{Weiss1998,
abstract = {Learning to predict rare events from sequences of events with categorical features is an important, real-world, problem that existing statistical and machine learning methods are not well suited to solve. This paper describes timeweaver, a genetic algorithm based machine learning system that predicts rare events by identifying predictive temporal and sequential patterns. Timeweaver is applied to the task of predicting telecommunication equipment failures from 110,000 alarm messages and is shown to outperform existing learning methods.},
author = {Weiss, Gary M and Hirsh, Haym},
doi = {10.1.1.30.8264},
journal = {Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining},
title = {{Learning to Predict Rare Events in Event Sequences}},
year = {1998}
}
@inproceedings{Zhang2017,
author = {Zhang, Shengdong and Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2017.7953302},
isbn = {978-1-5090-4117-6},
month = {mar},
pages = {5970--5974},
publisher = {IEEE},
title = {{Deep learning on symbolic representations for large-scale heterogeneous time-series event prediction}},
url = {http://ieeexplore.ieee.org/document/7953302/},
year = {2017}
}
@article{Swersky2016,
abstract = {—It has been shown that unsupervised outlier de-tection methods can be adapted to the one-class classification problem. In this paper, we focus on the comparison of one-class classification algorithms with such adapted unsupervised outlier detection methods, improving on previous comparison studies in several important aspects. We study a number of one-class classification and unsupervised outlier detection methods in a rigorous experimental setup, comparing them on a large number of datasets with different characteristics, using different performance measures. Our experiments led to conclusions that do not fully agree with those of previous work.},
author = {Swersky, Lorne and Marques, Henrique O and Campello, Ricardo J G B and Zimek, Arthur},
doi = {10.1109/DSAA.2016.8},
file = {:home/ander/Documentos/Bibliografia/swersky2016.pdf:pdf},
isbn = {9781509052066},
journal = {IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
keywords = {-semi-supervised learning,algorithms,evaluation,machine learning,one-class classification,outlier detection,predictive models,unsupervised learning},
pages = {1--10},
title = {{On the Evaluation of Outlier Detection and One-Class Classification Methods}},
year = {2016}
}

@article{Erfani2016,
abstract = {High-dimensional problem domains pose significant challenges for anomaly detection. The presence of irrelevant features can conceal the presence of anomalies. This problem, known as the 'curse of dimensionality', is an obstacle for many anomaly detection techniques. Building a robust anomaly detection model for use in high-dimensional spaces requires the combination of an unsupervised feature extractor and an anomaly detector. While one-class support vector machines are effective at producing decision surfaces from well-behaved feature vectors, they can be inefficient at modelling the variation in large, high-dimensional datasets. Architectures such as deep belief networks (DBNs) are a promising technique for learning robust features. We present a hybrid model where an unsupervised DBN is trained to extract generic underlying features, and a one-class SVM is trained from the features learned by the DBN. Since a linear kernel can be substituted for nonlinear ones in our hybrid model without loss of accuracy, our model is scalable and computationally efficient. The experimental results show that our proposed model yields comparable anomaly detection performance with a deep autoencoder, while reducing its training and testing time by a factor of 3 and 1000, respectively.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Erfani, Sarah M. and Rajasegarar, Sutharshan and Karunasekera, Shanika and Leckie, Christopher},
doi = {10.1016/j.patcog.2016.03.028},
eprint = {arXiv:1312.6184v5},
isbn = {9780521835688},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Anomaly detection,Deep belief net,Deep learning,Feature extraction,High-dimensional data,One-class SVM,Outlier detection},
mendeley-groups = {Trabajos/Review},
pmid = {27007977},
title = {{High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning}},
year = {2016}
}
@article{Khreich2017,
abstract = {Context: Run-time detection of system anomalies at the host level remains a challenging task. Existing techniques suffer from high rates of false alarms, hindering large-scale deployment of anomaly detection techniques in commercial settings.Objective: To reduce the false alarm rate, we present a new anomaly detection system based on a novel feature extraction technique, which combines the frequency with the temporal information from system call traces, and on one-class support vector machine (OC-SVM) detector.Method: The proposed feature extraction approach starts by segmenting the system call traces into multiple n-grams of variable length and mapping them to fixed-size sparse feature vectors, which are then used to train OC-SVM detectors.Results: The results achieved on a real-world system call dataset show that our feature vectors with up to 6-grams outperform the term vector models (using the most common weighting schemes) proposed in related work. More importantly, our anomaly detection system using OC-SVM with a Gaussian kernel, trained on our feature vectors, achieves a higher-level of detection accuracy (with a lower false alarm rate) than that achieved by Markovian and n-gram based models as well as by the state-of-the-art anomaly detection techniques.Conclusion: The proposed feature extraction approach from traces of events provides new and general data representations that are suitable for training standard one-class machine learning algorithms, while preserving the temporal dependencies among these events.},
author = {Khreich, Wael and Khosravifar, Babak and Hamou-Lhadj, Abdelwahab and Talhi, Chamseddine},
doi = {10.1016/j.infsof.2017.07.009},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Anomaly detection systems,Feature extraction,Intrusion detection and prevention,Software security,System calls,Tracing},
mendeley-groups = {Trabajos/Review},
title = {{An anomaly detection system based on variable N-gram features and one-class SVM}},
year = {2017}
}
@article{Dufrenois2016,
abstract = {Recently in Dufrenois [1], a new Fisher type contrast measure has been proposed to extract a target population in a dataset contaminated by outliers. Although mathematically sound, this work presents some further shortcomings in both the formalism and the field of use. First, we propose to re-express this problem from the formalism of proximal support vector machines as introduced in Mangasarian and Wild [2]. This change is far from harmless since it introduces a suited writing for solving the problem. Another limiting factor of the method is that its performance relies on the assumption that the density between the target and outliers are different. This consideration can easily prove to be over-optimistic for real world datasets making the method unreliable, at least directly. The computation of the decision boundary is a time consuming part of the algorithm since it is based on solving a generalized eigenvalue problem (GEP). This method is therefore limited to medium sized data sets. In this paper, we propose appropriate strategies to unlock all these shortcomings and fully benefit from the interest of the approach. Firstly, we show under some conditions that generating appropriate artificial outliers allows to stay within the constraints of the method and thus enlarges the conditions of use. Secondly, we show that the GEP can be advantageously replaced by a conjugate gradient solution (CG) significantly decreasing the computational cost. Lastly, the proposed algorithm is compared with recent novelty detectors on synthetic and real datasets.},
author = {Dufrenois, F. and Noyer, J. C.},
doi = {10.1016/j.patcog.2015.09.036},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Contrast measure,Linear programming problem,Outlier detection,Proximal support vector machines},
mendeley-groups = {Trabajos/Review},
title = {{One class proximal support vector machines}},
year = {2016}
}
@article{Swarnkar2016,
abstract = {Application specific attack detection requires packet payload analysis. Current payload analysis techniques suffer from failed detection as they use only the presence or absence of short sequences of a packet in a knowledge-base created out of non-malicious packets. In this paper, we describe OCPAD a content anomaly detection method to identify network packets with suspicious payload content. Proposed method combines the benefits of one class classification and frequency information of short sequences. We adapt one class Multinomial Naive Bayes classifier as anomaly detector for detecting HTTP attacks. OCPAD uses likelihood of each short sequence's occurrence in a payload of known non-malicious packets as a measure to derive the degree of maliciousness of a packet. In the training phase, OCPAD generates the likelihood range of each sequence's occurrence from every packet. In order to store the likelihood range of these sequences, we propose a novel and efficient data structure called Probability Tree. In the testing phase, it treats a short sequence as anomalous if it is not found in the database or its likelihood of occurrence in a packet is not in the range found in training phase. Using the likelihood of anomalous short sequences, it generates a class label for a test packet. Our experiments with a large dataset of 1 million HTTP packets collected from an academic network revealed OCPAD has a high Detection Rate (up to 100{\%}) compared to previous methods and acceptable rate of False Positives (less than 0.6{\%}).},
author = {Swarnkar, Mayank and Hubballi, Neminath},
doi = {10.1016/j.eswa.2016.07.036},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Anomaly detection,Intrusion detection,Payload analysis},
mendeley-groups = {Trabajos/Review},
title = {{OCPAD: One class Naive Bayes classifier for payload based anomaly detection}},
year = {2016}
}
@article{Ogbechie2017,
address = {Berlin, Heidelberg},
author = {Ogbechie, Alberto and D{\'{i}}az-Rozo, Javier and Larra{\~{n}}aga, Pedro and Bielza, Concha},
doi = {10.1007/978-3-662-53806-7_3},
file = {:home/ander/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ogbechie et al. - 2017 - Dynamic Bayesian Network-Based Anomaly Detection for In-Process Visual Inspection of Laser Surface Heat Treatme.pdf:pdf},
journal = {Machine Learning for Cyber Physical Systems},
mendeley-groups = {Trabajos/Review},
pages = {17--24},
publisher = {Springer Berlin Heidelberg},
title = {{Dynamic Bayesian Network-Based Anomaly Detection for In-Process Visual Inspection of Laser Surface Heat Treatment}},
url = {http://link.springer.com/10.1007/978-3-662-53806-7{\_}3},
year = {2017}
}
@article{Cadini2017,
abstract = {Cascading failures seriously threat the reliability/availability of power transmission networks. In fact, although rare, their consequences may be catastrophic, including large-scale blackouts affecting the economics and the social safety of entire regions. In this context, the quantification of the probability of occurrence of these events, as a consequence of the operating and environmental uncertain conditions, represents a fundamental task. To this aim, the classical simulation-based Monte Carlo (MC) approaches may be impractical, due to the fact that (i) power networks typically have very large reliabilities, so that cascading failures are rare events and (ii) very large computational expenses are required for the resolution of the cascading failure dynamics of real grids. In this work we originally propose to resort to two MC variance reduction techniques based on metamodeling for a fast approximation of the probability of occurrence of cascading failures leading to power losses. A new algorithm for properly initializing the variance reduction methods is also proposed, which is based on a smart Latin Hypercube search of the events of interest in the space of the uncertain inputs. The combined methods are demonstrated with reference to the realistic case study of a modified RTS 96 power transmission network of literature.},
author = {Cadini, Francesco and Agliardi, Gian Luca and Zio, Enrico},
doi = {10.1016/j.ress.2016.09.009},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Cascading failures,Kriging,Latin Hypercube,Monte Carlo,Power transmission networks,Rare event probabilities},
mendeley-groups = {Trabajos/Review},
title = {{Estimation of rare event probabilities in power transmission networks subject to cascading failures}},
year = {2017}
}
@article{Cheon2009,
abstract = {A Bayesian network is a powerful graphical model. It is advantageous for real-world data analysis and finding relations among variables. Knowledge presentation and rule generation, based on a Bayesian approach, have been studied and reported in many research papers across various fields. Since a Bayesian network has both causal and probabilistic semantics, it is regarded as an ideal representation to combine background knowledge and real data. Rare event predictions have been performed using several methods, but remain a challenge. We design and implement a Bayesian network model to forecast daily ozone states. We evaluate the proposed Bayesian network model, comparing it to traditional decision tree models, to examine its utility.},
author = {Cheon, Seong-Pyo and Kim, Sungshin and Lee, So-Young and Lee, Chong-Bum},
doi = {10.1016/j.knosys.2009.02.004},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Bayesian network,Ozone forecasting,Rare event prediction},
mendeley-groups = {Trabajos/Review},
title = {{Bayesian networks based rare event prediction with sensor data}},
year = {2009}
}
@article{Wu2003,
abstract = {Face detection is a canonical example of a rare event detection prob-lem, in which target patterns occur with much lower frequency than non-targets. Out of millions of face-sized windows in an input image, for ex-ample, only a few will typically contain a face. Viola and Jones recently proposed a cascade architecture for face detection which successfully ad-dresses the rare event nature of the task. A central part of their method is a feature selection algorithm based on AdaBoost. We present a novel cascade learning algorithm based on forward feature selection which is two orders of magnitude faster than the Viola-Jones approach and yields classifiers of equivalent quality. This faster method could be used for more demanding classification tasks, such as on-line learning.},
author = {Wu, Jianxin and Rehg, James M and Mullin, Matthew D},
file = {:home/ander/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Rehg, Mullin - 2003 - Learning a Rare Event Detection Cascade by Direct Feature Selection.pdf:pdf},
journal = {Neural Information Processing Systems (NIPS)},
mendeley-groups = {Trabajos/Review},
title = {{Learning a Rare Event Detection Cascade by Direct Feature Selection}},
url = {https://papers.nips.cc/paper/2353-learning-a-rare-event-detection-cascade-by-direct-feature-selection.pdf},
volume = {16},
year = {2003}
}
@article{Luca2016,
abstract = {Novelty detection or one-class classification starts from a model describing some type of 'normal behaviour' and aims to classify deviations from this model as being either novelties or anomalies. In this paper the problem of novelty detection for point patterns S = {\{}x 1 , . . . , x k {\}} ⊂ R d is treated where examples of anomalies are very sparse, or even absent. The latter complicates the tuning of hyperparameters in models commonly used for novelty detection, such as one-class support vector machines and hidden Markov models. To this end, the use of extreme value statistics is introduced to estimate explicitly a model for the abnormal class by means of extrapolation from a statistical model X for the normal class. We show how multiple types of information obtained from any available extreme instances of S can be combined to reduce the high false-alarm rate that is typically encountered when classes are strongly imbalanced, as often occurs in the one-class setting (whereby 'abnormal' data are often scarce). The approach is illustrated using simulated data and then a real-life application is used as an exemplar, whereby accelerometry data from epileptic seizures are analysed -these are known to be extreme and rare with respect to normal accelerometer data.},
author = {Luca, Stijn and Clifton, David A and Vanrumste, Bart},
file = {:home/ander/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luca, Clifton, Vanrumste - 2016 - One-class classification of point patterns of extremes(2).pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Sequence classification,asymptotic theory,class imbal-ance,extreme value theory,novelty detection},
mendeley-groups = {Trabajos/Review},
pages = {1--21},
title = {{One-class classification of point patterns of extremes}},
url = {http://jmlr.org/papers/volume17/16-112/16-112.pdf},
volume = {17},
year = {2016}
}
@article{Domingues2018,
abstract = {We survey unsupervised machine learning algorithms in the context of outlier detection. This task challenges state-of-the-art methods from a variety of research fields to applications including fraud detection, intrusion detection, medical diagnoses and data cleaning. The selected methods are benchmarked on publicly available datasets and novel industrial datasets. Each method is then submitted to extensive scalability, memory consumption and robustness tests in order to build a full overview of the algorithms' characteristics.},
author = {Domingues, R{\'{e}}mi and Filippone, Maurizio and Michiardi, Pietro and Zouaoui, Jihane},
doi = {10.1016/J.PATCOG.2017.09.037},
issn = {0031-3203},
journal = {Pattern Recognition},
mendeley-groups = {Trabajos/Review},
month = {feb},
pages = {406--421},
publisher = {Pergamon},
title = {{A comparative evaluation of outlier detection algorithms: Experiments and analyses}},
url = {https://www.sciencedirect.com/science/article/pii/S0031320317303916},
volume = {74},
year = {2018}
}
@article{Martinez-Rego2016,
abstract = {Predictive maintenance has emerged as a fundamental practice to preserve production assets in many industrial environments. Of a wide set of approaches, vibration analysis is one of the most used for high-speed rotating machinery, especially when fault detection is to be automatic. Traditionally, this task has been studied as a classification problem using data extracted from the frequency domain. This approach, however, has two main limitations: (a) manufacture and mounting procedures can vary the vibration spectra of a machine, even when these share the same design; and (b) incipient fault signatures may be concealed in the frequency domain by noise and vibration from other parts of the system. For these reasons, the application of a classifier obtained for one machine to another machine is pointless, making early fault detection difficult. In this paper, a bearing fault detection problem is tackled using one-class classifiers and features extracted from vibration capture in the time domain using recurrence time statistics. We also describe a study of the behavior of the proposed method in real conditions. Our method shows high detection accuracy accompanied by a reduced number of false positives and negatives.},
author = {Mart{\'{i}}nez-Rego, David and Fontenla-Romero, Oscar and Alonso-Betanzos, Amparo and Principe, Jos{\'{e}} C.},
doi = {10.1016/J.PATREC.2016.07.019},
issn = {0167-8655},
journal = {Pattern Recognition Letters},
mendeley-groups = {Trabajos/Review},
month = {dec},
pages = {8--14},
publisher = {North-Holland},
title = {{Fault detection via recurrence time statistics and one-class classification}},
url = {https://www.sciencedirect.com/science/article/pii/S0167865516301817},
volume = {84},
year = {2016}
}
@article{Martinez-Rego2016,
abstract = {Predictive maintenance has emerged as a fundamental practice to preserve production assets in many industrial environments. Of a wide set of approaches, vibration analysis is one of the most used for high-speed rotating machinery, especially when fault detection is to be automatic. Traditionally, this task has been studied as a classification problem using data extracted from the frequency domain. This approach, however, has two main limitations: (a) manufacture and mounting procedures can vary the vibration spectra of a machine, even when these share the same design; and (b) incipient fault signatures may be concealed in the frequency domain by noise and vibration from other parts of the system. For these reasons, the application of a classifier obtained for one machine to another machine is pointless, making early fault detection difficult. In this paper, a bearing fault detection problem is tackled using one-class classifiers and features extracted from vibration capture in the time domain using recurrence time statistics. We also describe a study of the behavior of the proposed method in real conditions. Our method shows high detection accuracy accompanied by a reduced number of false positives and negatives.},
author = {Mart{\'{i}}nez-Rego, David and Fontenla-Romero, Oscar and Alonso-Betanzos, Amparo and Principe, Jos{\'{e}} C.},
doi = {10.1016/J.PATREC.2016.07.019},
issn = {0167-8655},
journal = {Pattern Recognition Letters},
mendeley-groups = {Trabajos/Review},
month = {dec},
pages = {8--14},
publisher = {North-Holland},
title = {{Fault detection via recurrence time statistics and one-class classification}},
url = {https://www.sciencedirect.com/science/article/pii/S0167865516301817},
volume = {84},
year = {2016}
}
@article{Noto2012,
abstract = {Anomaly detection involves identifying rare data instances (anomalies) that come from a different class or distribution than the majority (which are simply called "normal" instances). Given a training set of only normal data, the semi-supervised anomaly detection task is to identify anomalies in the future. Good solutions to this task have applications in fraud and intrusion detection. The unsupervised anomaly detection task is different: Given unlabeled, mostly-normal data, identify the anomalies among them. Many real-world machine learning tasks, including many fraud and intrusion detection tasks, are unsupervised because it is impractical (or impossible) to verify all of the training data. We recently presented FRaC, a new approach for semi-supervised anomaly detection. FRaC is based on using normal instances to build an ensemble of feature models, and then identifying instances that disagree with those models as anomalous. In this paper, we investigate the behavior of FRaC experimentally and explain why FRaC is so successful. We also show that FRaC is a superior approach for the unsupervised as well as the semi-supervised anomaly detection task, compared to well-known state-of-the-art anomaly detection methods, LOF and one-class support vector machines, and to an existing feature-modeling approach.},
author = {Noto, Keith and Brodley, Carla and Slonim, Donna},
doi = {10.1007/s10618-011-0234-x},
file = {:home/ander/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Noto, Brodley, Slonim - 2012 - FRaC a feature-modeling approach for semi-supervised and unsupervised anomaly detection.pdf:pdf},
issn = {1384-5810},
journal = {Data mining and knowledge discovery},
number = {1},
pages = {109--133},
pmid = {22639542},
publisher = {NIH Public Access},
title = {{FRaC: a feature-modeling approach for semi-supervised and unsupervised anomaly detection.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22639542 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3359096},
volume = {25},
year = {2012}
}
@article{Ribeiro2016,
author = {Ribeiro, Rita P. and Pereira, Pedro and Gama, Jo{\~{a}}o},
doi = {10.1007/s10994-016-5584-6},
file = {:home/ander/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Pereira, Gama - 2016 - Sequential anomalies a study in the Railway Industry.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
mendeley-groups = {Trabajos/Review},
month = {oct},
number = {1},
pages = {127--153},
publisher = {Springer US},
title = {{Sequential anomalies: a study in the Railway Industry}},
url = {http://link.springer.com/10.1007/s10994-016-5584-6},
volume = {105},
year = {2016}
}
@inproceedings{XuanHongDang2014,
author = {{Xuan Hong Dang} and Assent, Ira and Ng, Raymond T. and Zimek, Arthur and Schubert, Erich},
booktitle = {2014 IEEE 30th International Conference on Data Engineering},
doi = {10.1109/ICDE.2014.6816642},
isbn = {978-1-4799-2555-1},
mendeley-groups = {Trabajos/Review},
month = {mar},
pages = {88--99},
publisher = {IEEE},
title = {{Discriminative features for identifying and interpreting outliers}},
url = {http://ieeexplore.ieee.org/document/6816642/},
year = {2014}
}
